{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/35887/logos/header.png?t=2022-05-09-22-33-02\">\n\n<h1><center>[2/3] AI4Code TensorFlow TPU with CodeBert - Training</center></h1>\n\nThis is the second part of my **AI4Code TensorFlow TPU with CodeBert** series:\n\n* [1/3] [Data Preparation][1] (~5 hours)\n* **[2/3] TPU Training ← (you're here)**\n* [3/3] [GPU Inference][2] (~2 hours)\n\nThis is basically a translation of **[Khoi Nguyen's][3]** works [[1][4], [2][5]] from PyTorch to TensorFlow with minor changes and updates for TPU support. The **[original][4]** PyTorch work takes up to 40 hours per epoch on Kaggle GPU, whereas this version takes only 50 minutes per epoch on Kaggle TPU, so it's lightning fast ⚡.\n\nModel weights are already saved to the dataset **[AI4Code CodeBert Weights][6]**.\n\n### About Solution\n\n- Input data: markdown + code context (512 tokens) + features\n    - Markdown (up to 64 tokens)\n    - Code context (all code cells or up to 20 code cells each up to 23 tokens)\n    - Features: markdown cells to total cells ratio (appended to backbone outputs)\n- Model and hyperparameters\n    - CodeBert Base model\n    - L1 loss (MAE)\n    - AdamW optimizer\n    - Learning rate schedule with warmup and linear decay\n    - Total 5 epochs\n\n### Input Data\n\n- **[AI4Code-CodeBert-Tokens][7]**: output from **[Data Preparation][1]** step\n\n### Warning\n\nThis notebook uses Kaggle environment variables. If you run it on Google Colab make sure you explicitly set `VERBOSE` hyperparameter to either 1 or 2.\n\n[1]: https://www.kaggle.com/nickuzmenkov/ai4code-tf-tpu-codebert-data-preparation\n[2]: https://www.kaggle.com/nickuzmenkov/ai4code-tf-tpu-codebert-inference\n[3]: https://www.kaggle.com/suicaokhoailang\n[4]: https://github.com/suicao/ai4code-baseline/tree/main/code\n[5]: https://www.kaggle.com/code/suicaokhoailang/stronger-baseline-with-code-cells\n[6]: https://www.kaggle.com/datasets/nickuzmenkov/ai4code-codebert-weights\n[7]: https://www.kaggle.com/datasets/nickuzmenkov/ai4code-codebert-tokens\n\n# Setup","metadata":{}},{"cell_type":"code","source":"import os\nfrom typing import List\n\nimport numpy as np\nimport tensorflow as tf\nimport transformers\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'","metadata":{"execution":{"iopub.status.busy":"2022-07-09T02:03:28.968935Z","iopub.execute_input":"2022-07-09T02:03:28.969300Z","iopub.status.idle":"2022-07-09T02:03:37.985102Z","shell.execute_reply.started":"2022-07-09T02:03:28.969212Z","shell.execute_reply":"2022-07-09T02:03:37.983772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_STATE = 42\nN_SPLITS = 5\nTOTAL_MAX_LEN = 512\nBASE_MODEL = \"microsoft/codebert-base\"\nGCS_PATH = KaggleDatasets().get_gcs_path(\"ai4code-codebert-tokens\")\nEPOCHS = 5\nLR = 3e-5\nWARMUP_RATE = 0.05\nVERBOSE = 1 if os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\" else 2\n\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\n    BATCH_SIZE = 64 * STRATEGY.num_replicas_in_sync\nexcept Exception:\n    TPU = None\n    STRATEGY = tf.distribute.get_strategy()\n    BATCH_SIZE = 4\n\nprint(\"TensorFlow\", tf.__version__)\n\nif TPU is not None:\n    print(\"Using TPU v3-8\")\nelse:\n    print(\"Using GPU/CPU\")\n\nprint(\"Batch size:\", BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T02:03:37.987403Z","iopub.execute_input":"2022-07-09T02:03:37.987792Z","iopub.status.idle":"2022-07-09T02:03:44.923643Z","shell.execute_reply.started":"2022-07-09T02:03:37.987759Z","shell.execute_reply":"2022-07-09T02:03:44.922330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_samples(filenames: List[str]) -> int:\n    return sum(int(os.path.basename(x).split(\".\")[0].split(\"-\")[-1]) for x in filenames)\n\n\ndef read_tfrecord(example):\n    features = {\n        \"input_ids\": tf.io.FixedLenFeature(\n            [\n                TOTAL_MAX_LEN,\n            ],\n            tf.int64,\n        ),\n        \"attention_mask\": tf.io.FixedLenFeature(\n            [\n                TOTAL_MAX_LEN,\n            ],\n            tf.int64,\n        ),\n        \"feature\": tf.io.FixedLenFeature([], tf.float32),\n        \"label\": tf.io.FixedLenFeature([], tf.float32),\n    }\n    example = tf.io.parse_single_example(example, features)\n    return (\n        {\n            \"input_ids\": tf.cast(example[\"input_ids\"], tf.int32),\n            \"attention_mask\": tf.cast(example[\"attention_mask\"], tf.int32),\n            \"feature\": example[\"feature\"],\n        },\n        example[\"label\"],\n    )\n\n\ndef get_dataset(\n    filenames: List[str],\n    ordered: bool = False,\n    repeated: bool = True,\n    cached: bool = False,\n) -> tf.data.Dataset:\n    auto = tf.data.experimental.AUTOTUNE\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=auto)\n    if not ordered:\n        ignore_order = tf.data.Options()\n        ignore_order.experimental_deterministic = False\n        dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=auto)\n    if not ordered:\n        dataset = dataset.shuffle(2048, seed=RANDOM_STATE)\n    if repeated:\n        dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n    if cached:\n        dataset = dataset.cache()\n    dataset = dataset.prefetch(auto)\n    return STRATEGY.experimental_distribute_dataset(dataset)\n\n\ndef get_model() -> tf.keras.Model:\n    backbone = transformers.TFAutoModel.from_pretrained(BASE_MODEL)\n    input_ids = tf.keras.layers.Input(\n        shape=(TOTAL_MAX_LEN,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    attention_mask = tf.keras.layers.Input(\n        shape=(TOTAL_MAX_LEN,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    feature = tf.keras.layers.Input(\n        shape=(1,),\n        dtype=tf.float32,\n        name=\"feature\",\n    )\n    x = backbone({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[0]\n    x = tf.concat([x[:, 0, :], feature], axis=1)\n    outputs = tf.keras.layers.Dense(1, activation=\"linear\", dtype=\"float32\")(x)\n    return tf.keras.Model(\n        inputs=[input_ids, attention_mask, feature],\n        outputs=outputs,\n    )\n\n\nclass WarmupLinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(\n        self,\n        base_learning_rate: float,\n        warmup_steps: int,\n        total_steps: int,\n    ) -> None:\n        self._base_learning_rate = base_learning_rate\n        self._warmup_steps = warmup_steps\n        self._total_steps = total_steps\n\n    def __call__(self, step: int) -> float:\n        return self._base_learning_rate * tf.cond(\n            tf.math.less_equal(step, warmup_steps),\n            lambda: step / self._warmup_steps,\n            lambda: (step - total_steps) / (self._warmup_steps - self._total_steps),\n        )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-07-09T02:03:44.925609Z","iopub.execute_input":"2022-07-09T02:03:44.925900Z","iopub.status.idle":"2022-07-09T02:03:44.952647Z","shell.execute_reply.started":"2022-07-09T02:03:44.925869Z","shell.execute_reply":"2022-07-09T02:03:44.951503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"비율 나눈 걸 따로 할려면 여기 부분을 좀 바꿔야 할 거 같은데 잘 한건지는 모르겠지만 일단 했다. ","metadata":{}},{"cell_type":"code","source":"for i, (train_index, val_index) in enumerate(KFold(n_splits=N_SPLITS).split(range(N_SPLITS))):\n    if TPU is not None:\n        tf.tpu.experimental.initialize_tpu_system(TPU)\n\n    train_filenames = np.ravel(   #이게 1번\n        [\n            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec\", str(x), \"*.tfrec\"))\n            for x in train_index\n        ]\n    )\n    steps_per_epoch = count_samples(train_filenames) // BATCH_SIZE\n    train_dataset = get_dataset(train_filenames)\n    \n\n    val_filenames = np.ravel(                           \n        [\n            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec\", str(x), \"*.tfrec\"))\n            for x in val_index\n        ]\n    )\n    validation_steps = count_samples(val_filenames) // BATCH_SIZE\n    val_dataset = get_dataset(val_filenames, ordered=True, repeated=False, cached=True)\n    \n\n    with STRATEGY.scope():\n        model = get_model()\n\n        total_steps = steps_per_epoch * EPOCHS\n        warmup_steps = int(WARMUP_RATE * total_steps)\n\n        optimizer = transformers.AdamWeightDecay(\n            learning_rate=WarmupLinearDecay(\n                base_learning_rate=LR,\n                warmup_steps=warmup_steps,\n                total_steps=total_steps,\n            ),\n            weight_decay_rate=0.01,\n            exclude_from_weight_decay=[\n                \"bias\",\n                \"LayerNorm.bias\",\n                \"LayerNorm.weight\",\n            ],\n        )\n        model.compile(loss=\"mae\", optimizer=optimizer)\n\n    model.fit(                  \n        train_dataset,\n        steps_per_epoch=steps_per_epoch,\n        validation_data=val_dataset,\n        validation_steps=validation_steps,\n        epochs=EPOCHS,\n        verbose=VERBOSE,\n    )\n\n    model.save_weights(f\"model_{i}.h5\")\n    break","metadata":{"execution":{"iopub.status.busy":"2022-07-09T02:03:44.954792Z","iopub.execute_input":"2022-07-09T02:03:44.955104Z","iopub.status.idle":"2022-07-09T06:18:05.243290Z","shell.execute_reply.started":"2022-07-09T02:03:44.955017Z","shell.execute_reply":"2022-07-09T06:18:05.241481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위에는 1번 거, 아래는 2번 거. 나눠서 하는 게 더 나은 것 같아서 이렇게 해보기.","metadata":{}},{"cell_type":"code","source":"for i, (train_index, val_index) in enumerate(KFold(n_splits=N_SPLITS).split(range(N_SPLITS))):\n    if TPU is not None:\n        tf.tpu.experimental.initialize_tpu_system(TPU)\n    \n    train_filenames2 = np.ravel( # 새로 만들어준 거. 근데 x 대신 다른 알파벳을 사용해야할지 말지 모르겠어서 일단 두기.\n        [\n            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec2\", str(x), \"*.tfrec2\"))\n            for x in train_index\n        ]\n    )\n    steps_per_epoch2 = count_samples(train_filenames2) // BATCH_SIZE\n    train_dataset2 = get_dataset(train_filenames2)\n    \n    val_filenames2 = np.ravel( #새로 만들어 준거. 만들긴 했는데 그냥 이걸 아예 같은 걸 이름만 바꿔서 새로 만들어야 할지는 모르겠음. 근데 바꾸면 안될것 같아서.\n        [\n            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec2\", str(x), \"*.tfrec2\"))\n            for x in val_index\n        ]\n    )\n    validation_steps2 = count_samples(val_filenames2) // BATCH_SIZE\n    val_dataset2 = get_dataset(val_filenames2, ordered=True, repeated=False, cached=True)\n\n    with STRATEGY.scope():\n        model = get_model()\n\n        total_steps2 = steps_per_epoch2 * EPOCHS\n        warmup_steps2 = int(WARMUP_RATE * total_steps)\n\n        optimizer = transformers.AdamWeightDecay( # 여기도 2를 넣어 줘야 하는 지 잘 모르겠다. \n            learning_rate=WarmupLinearDecay(\n                base_learning_rate=LR,\n                warmup_steps=warmup_steps,\n                total_steps=total_steps,\n            ),\n            weight_decay_rate=0.01,\n            exclude_from_weight_decay=[\n                \"bias\",\n                \"LayerNorm.bias\",\n                \"LayerNorm.weight\",\n            ],\n        )\n        model.compile(loss=\"mae\", optimizer=optimizer)\n\n    model.fit(                                    # 여기도 2를 넣어 줘야 하는 지 잘 모르겠다. \n        train_dataset2,\n        steps_per_epoch2=steps_per_epoch2,\n        validation_data2=val_dataset2,\n        validation_steps2=validation_steps2,\n        epochs=EPOCHS,\n        verbose=VERBOSE,\n    )\n\n    model.save_weights(f\"model_{i}.h5\")#여기 부분이 의문점. 뭔가 바꿔야 하는 것 같은데 잘 모르겠다. 아닐 수도...   \n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Steps\n\nGo to the model weights dataset **[here][1]** or continue exploring:\n\n* [1/3] [Data Preparation][2] (~3 hours)\n* <span style=\"color:lightgray\">[2/3] TPU Training ← (you're here)</span>\n* [3/3] [GPU Inference][3] (~2 hours)\n\n\n[1]: https://www.kaggle.com/datasets/nickuzmenkov/ai4code-codebert-weights\n[2]: https://www.kaggle.com/nickuzmenkov/ai4code-tf-tpu-codebert-data-preparation\n[3]: https://www.kaggle.com/nickuzmenkov/ai4code-tf-tpu-codebert-inference","metadata":{}}]}