{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI4Code TF TPU CodeBert - Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGpZYI+cz0aQik9zYtCWTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonseimath/datascience-biginner-2022-kaggle-competitions/blob/feature%2Fyenakim/yenakim/AI4Code_TF_TPU_CodeBert_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "A_8yENSFhRWB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz362r6xtD9B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "N_SPLITS = 5\n",
        "TOTAL_MAX_LEN = 512\n",
        "BASE_MODEL = \"microsoft/codebert-base\"\n",
        "GCS_PATH = KaggleDatasets().get_gcs_path(\"ai4code-codebert-tokens\")\n",
        "EPOCHS = 5\n",
        "LR = 3e-5\n",
        "WARMUP_RATE = 0.05\n",
        "VERBOSE = 1 if os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\" else 2 # 1은 자세한 출력, 2는 함축적인 출력\n",
        "\n",
        "# TPU 사용\n",
        "try:\n",
        "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(TPU)\n",
        "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\n",
        "    BATCH_SIZE = 64 * STRATEGY.num_replicas_in_sync\n",
        "except Exception:\n",
        "    TPU = None\n",
        "    STRATEGY = tf.distribute.get_strategy()\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "print(\"TensorFlow\", tf.__version__)\n",
        "\n",
        "if TPU is not None:\n",
        "    print(\"Using TPU v3-8\")\n",
        "else:\n",
        "    print(\"Using GPU/CPU\")\n",
        "\n",
        "print(\"Batch size:\", BATCH_SIZE)"
      ],
      "metadata": {
        "id": "x_hfZ0cptsim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 수 세기\n",
        "def count_samples(filenames: List[str]) -> int:\n",
        "    return sum(int(os.path.basename(x).split(\".\")[0].split(\"-\")[-1]) for x in filenames)\n",
        "\n",
        "# Serialize된 파일을 입력받아 모델에서 사용할 수 있도록 형태를 변환\n",
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"input_ids\": tf.io.FixedLenFeature( # int64 배열의 문자열을 받는다는 의미\n",
        "            [\n",
        "                TOTAL_MAX_LEN,\n",
        "            ],\n",
        "            tf.int64,\n",
        "        ),\n",
        "        \"attention_mask\": tf.io.FixedLenFeature(\n",
        "            [\n",
        "                TOTAL_MAX_LEN,\n",
        "            ],\n",
        "            tf.int64,\n",
        "        ),\n",
        "        \"feature\": tf.io.FixedLenFeature([], tf.float32),\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.float32),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, features) # 입력받은 파일을 하나씩 풀어줌\n",
        "    return (\n",
        "        {\n",
        "            \"input_ids\": tf.cast(example[\"input_ids\"], tf.int32), # cast : 텐서를 새로운 형태로 캐스팅\n",
        "            \"attention_mask\": tf.cast(example[\"attention_mask\"], tf.int32),\n",
        "            \"feature\": example[\"feature\"],\n",
        "        },\n",
        "        example[\"label\"],\n",
        "    )\n",
        "\n",
        "\n",
        "def get_dataset(\n",
        "    filenames: List[str],\n",
        "    ordered: bool = False,\n",
        "    repeated: bool = True,\n",
        "    cached: bool = False,\n",
        ") -> tf.data.Dataset:\n",
        "    auto = tf.data.experimental.AUTOTUNE # 사용 가능한 CPU에 따라 병렬 호출 수가 동적으로 결정\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=auto)\n",
        "    if not ordered:\n",
        "        ignore_order = tf.data.Options()\n",
        "        ignore_order.experimental_deterministic = False # transformation is allowed to yield elements out of order to trade determinism for performance\n",
        "        dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=auto)\n",
        "    if not ordered:\n",
        "        dataset = dataset.shuffle(2048, seed=RANDOM_STATE)\n",
        "    if repeated:\n",
        "        dataset = dataset.repeat() # 데이터 셋 반복\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # 데이터 배치의 크기 설정\n",
        "    if cached:\n",
        "        dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(auto) # 데이터가 소비되는 시간과 데이터가 생성되는 시간 간의 의존성을 줄이는 변환\n",
        "    return STRATEGY.experimental_distribute_dataset(dataset) # 복제본 당 값 생성\n",
        "\n",
        "# 모델 만들기\n",
        "def get_model() -> tf.keras.Model:\n",
        "    backbone = transformers.TFAutoModel.from_pretrained(BASE_MODEL) # 사전학습된 code bert 불러옴\n",
        "    input_ids = tf.keras.layers.Input( # 층 생성\n",
        "        shape=(TOTAL_MAX_LEN,),\n",
        "        dtype=tf.int32,\n",
        "        name=\"input_ids\",\n",
        "    )\n",
        "    attention_mask = tf.keras.layers.Input(\n",
        "        shape=(TOTAL_MAX_LEN,),\n",
        "        dtype=tf.int32,\n",
        "        name=\"attention_mask\",\n",
        "    )\n",
        "    feature = tf.keras.layers.Input(\n",
        "        shape=(1,),\n",
        "        dtype=tf.float32,\n",
        "        name=\"feature\",\n",
        "    )\n",
        "    x = backbone({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[0]\n",
        "    x = tf.concat([x[:, 0, :], feature], axis=1)\n",
        "    outputs = tf.keras.layers.Dense(1, activation=\"linear\", dtype=\"float32\")(x)\n",
        "    return tf.keras.Model(\n",
        "        inputs=[input_ids, attention_mask, feature],\n",
        "        outputs=outputs,\n",
        "    )\n",
        "\n",
        "# learning rate 조절?\n",
        "class WarmupLinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_learning_rate: float,\n",
        "        warmup_steps: int,\n",
        "        total_steps: int,\n",
        "    ) -> None:\n",
        "        self._base_learning_rate = base_learning_rate\n",
        "        self._warmup_steps = warmup_steps\n",
        "        self._total_steps = total_steps\n",
        "\n",
        "    def __call__(self, step: int) -> float:\n",
        "        return self._base_learning_rate * tf.cond(\n",
        "            tf.math.less_equal(step, warmup_steps),\n",
        "            lambda: step / self._warmup_steps,\n",
        "            lambda: (step - total_steps) / (self._warmup_steps - self._total_steps),\n",
        "        )"
      ],
      "metadata": {
        "id": "7RLu3rYufpxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "H_05l5ZkhVAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (train_index, val_index) in enumerate(KFold(n_splits=N_SPLITS).split(range(N_SPLITS))):\n",
        "    if TPU is not None:\n",
        "        tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "\n",
        "    train_filenames = np.ravel( # 1차원 배열 반환\n",
        "        [\n",
        "            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec\", str(x), \"*.tfrec\"))\n",
        "            for x in train_index\n",
        "        ]\n",
        "    )\n",
        "    steps_per_epoch = count_samples(train_filenames) // BATCH_SIZE # 한 에포크 당 스텝\n",
        "    train_dataset = get_dataset(train_filenames)\n",
        "\n",
        "    val_filenames = np.ravel(\n",
        "        [\n",
        "            tf.io.gfile.glob(os.path.join(GCS_PATH, \"tfrec\", str(x), \"*.tfrec\"))\n",
        "            for x in val_index\n",
        "        ]\n",
        "    )\n",
        "    validation_steps = count_samples(val_filenames) // BATCH_SIZE\n",
        "    val_dataset = get_dataset(val_filenames, ordered=True, repeated=False, cached=True)\n",
        "\n",
        "    with STRATEGY.scope(): # model, optimizer, and checkpoint must be created under `strategy.scope`\n",
        "        model = get_model()\n",
        "\n",
        "        total_steps = steps_per_epoch * EPOCHS\n",
        "        warmup_steps = int(WARMUP_RATE * total_steps)\n",
        "\n",
        "        optimizer = transformers.AdamWeightDecay( # 옵티마이저 : AdamW\n",
        "            learning_rate=WarmupLinearDecay(\n",
        "                base_learning_rate=LR,\n",
        "                warmup_steps=warmup_steps,\n",
        "                total_steps=total_steps,\n",
        "            ),\n",
        "            weight_decay_rate=0.01,\n",
        "            exclude_from_weight_decay=[\n",
        "                \"bias\",\n",
        "                \"LayerNorm.bias\",\n",
        "                \"LayerNorm.weight\",\n",
        "            ],\n",
        "        )\n",
        "        model.compile(loss=\"mae\", optimizer=optimizer)\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=VERBOSE,\n",
        "    )\n",
        "\n",
        "    model.save_weights(f\"model_{i}.h5\")\n",
        "    break"
      ],
      "metadata": {
        "id": "b-omk47LhV9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}