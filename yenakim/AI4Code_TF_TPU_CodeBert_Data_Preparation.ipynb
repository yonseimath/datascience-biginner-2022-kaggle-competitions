{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI4Code TF TPU CodeBert - Data Preparation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdCxt2PJjhBzlfPUVd6LRs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonseimath/datascience-biginner-2022-kaggle-competitions/blob/feature%2Fyenakim/yenakim/AI4Code_TF_TPU_CodeBert_Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/nickuzmenkov/ai4code-tf-tpu-codebert-data-preparation/notebook의 코드를 공부한 노트북입니다."
      ],
      "metadata": {
        "id": "ZqIkLQ9LmzyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "pfG0p0YbcNFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRDbcDLIE2uE"
      },
      "outputs": [],
      "source": [
        "!mkdir 'raw' 'tfrec'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "8r4p4C5xFAHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "MD_MAX_LEN = 64 # 마크다운 토큰은 최대 64 토큰까지\n",
        "TOTAL_MAX_LEN = 512 # Code context는 512 토큰까지(23 토큰 이하로 구성된 코드 셀 20개까지)\n",
        "K_FOLDS = 5\n",
        "FILES_PER_FOLD = 16\n",
        "LIMIT = 1_000 if os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\" else None # KAGGLE_KERNEL_RUN_TYPE 환경 변수가 Interactive이면 앞의 1000개의 노트북만을사용한다\n",
        "# LIMIT = None # 전체 노트북 사용\n",
        "MODEL_NAME = \"microsoft/codebert-base\"\n",
        "TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "INPUT_PATH = \"../input/AI4Code\""
      ],
      "metadata": {
        "id": "BhIIElpEFHrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노트북의 경로를 변수로 받아 내용을 읽어온 후 DataFrame 형식으로 return\n",
        "def read_notebook(path: str) -> pd.DataFrame:\n",
        "    return (\n",
        "        pd.read_json(path, dtype={\"cell_type\": \"category\", \"source\": \"str\"})\n",
        "        .assign(id=os.path.basename(path).split(\".\")[0])\n",
        "        .rename_axis(\"cell_id\")\n",
        "    ) \n",
        "\n",
        "# 셀에 있는 \\n 코드가 \\\\n으로 입력되므로 replace 해주고 이를 다시 return\n",
        "def clean_code(cell: str) -> str:\n",
        "    return str(cell).replace(\"\\\\n\", \"\\n\")\n",
        "\n",
        "# n보다 cell들이 많으면 샘플을 뽑고, 그렇지 않으면 그대로 cells를 return\n",
        "def sample_cells(cells: List[str], n: int) -> List[str]:\n",
        "   cells = [clean_code(cell) for cell in cells] \n",
        "    if n >= len(cells):\n",
        "        return cells\n",
        "    else:\n",
        "        results = []\n",
        "        step = len(cells) / n\n",
        "        idx = 0\n",
        "        while int(np.round(idx)) < len(cells): # 전체 샘플 중 step만큼 건너뛴 샘플들을 골라 result에 넣음\n",
        "            results.append(cells[int(np.round(idx))])\n",
        "            idx += step\n",
        "        if cells[-1] not in results:\n",
        "            results[-1] = cells[-1] # 마지막 셀은 반드시 넣음\n",
        "        return results\n",
        "\n",
        "# total_code(전체 코드), total_md(전체 마크다운), codes(코드에서 샘플 추출) 특성 생성\n",
        "def get_features(df: pd.DataFrame) -> dict:\n",
        "    features = {}\n",
        "    for i, sub_df in tqdm(df.groupby(\"id\"), desc=\"Features\"):\n",
        "        features[i] = {}\n",
        "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
        "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
        "        total_code = code_sub_df.shape[0]\n",
        "        codes = sample_cells(code_sub_df.source.values, 20)\n",
        "        features[i][\"total_code\"] = total_code\n",
        "        features[i][\"total_md\"] = total_md\n",
        "        features[i][\"codes\"] = codes\n",
        "    return features\n",
        "\n",
        "# input 값을 준비\n",
        "def tokenize(df: pd.DataFrame, fts: dict) -> dict:\n",
        "    input_ids = np.zeros((len(df), TOTAL_MAX_LEN), dtype=np.int32)\n",
        "    attention_mask = np.zeros((len(df), TOTAL_MAX_LEN), dtype=np.int32)\n",
        "    features = np.zeros((len(df),), dtype=np.float32)\n",
        "    labels = np.zeros((len(df),), dtype=np.float32)\n",
        "\n",
        "    for i, row in tqdm( # tqdm : 진행률을 보여주는 바 생성\n",
        "        df.reset_index(drop=True).iterrows(), desc=\"Tokens\", total=len(df) \n",
        "    ): # iterrows : 행번호와 값 동시에 출력, reset_index : 인덱스 초기화\n",
        "        row_fts = fts[row.id]\n",
        "\n",
        "        inputs = TOKENIZER.encode_plus(\n",
        "            row.source,\n",
        "            None,\n",
        "            add_special_tokens=True, # 토큰의 시작점에 [CLS] 토큰, 토큰의 마지막에 [SEP] 토큰을 붙인다\n",
        "            max_length=MD_MAX_LEN,\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True, # token type id 생성(0과 1로 문장의 토큰 값 분리)\n",
        "            truncation=True,\n",
        "        ) # Markdown(~64)\n",
        "        code_inputs = TOKENIZER.batch_encode_plus(\n",
        "            [str(x) for x in row_fts[\"codes\"]] or [\"\"],\n",
        "            add_special_tokens=True,\n",
        "            max_length=23,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        ) # code context(~512)\n",
        "\n",
        "        ids = inputs[\"input_ids\"] # 토크나이즈 된 MD\n",
        "        for x in code_inputs[\"input_ids\"]:\n",
        "            ids.extend(x[:-1]) # 토크나이즈 된 코드를 ids 뒤에 이어붙임\n",
        "        ids = ids[:TOTAL_MAX_LEN] # 512 토큰까지만 사용\n",
        "        if len(ids) != TOTAL_MAX_LEN: # 토큰의 길이가 512보다 작으면 그만큼 pad_token_id를 이어붙임\n",
        "            ids = ids + [\n",
        "                TOKENIZER.pad_token_id,\n",
        "            ] * (TOTAL_MAX_LEN - len(ids))\n",
        "\n",
        "        mask = inputs[\"attention_mask\"] # 패딩된 부분이 학습에 영향을 주지 않도록 처리하는 입력값, 1은 어텐션에 영향을 받는 토큰이고 0은 영향을 받지 않는 토큰\n",
        "        for x in code_inputs[\"attention_mask\"]:\n",
        "            mask.extend(x[:-1]) # 코드 마스크도 뒤에 이어붙임\n",
        "        mask = mask[:TOTAL_MAX_LEN] # 512 토큰까지만 사용\n",
        "        if len(mask) != TOTAL_MAX_LEN: # 토큰의 길이가 512보다 작으면 그만큼 pad_token_id를 이어붙임\n",
        "            mask = mask + [\n",
        "                TOKENIZER.pad_token_id,\n",
        "            ] * (TOTAL_MAX_LEN - len(mask))\n",
        "\n",
        "        input_ids[i] = ids # 결과 배열에 넣음\n",
        "        attention_mask[i] = mask\n",
        "        features[i] = (\n",
        "            row_fts[\"total_md\"] / (row_fts[\"total_md\"] + row_fts[\"total_code\"]) or 1 # 분모가 1일 때 예외처리?\n",
        "        ) # 특성은 MD 셀 수 / 전체 셀 수\n",
        "        labels[i] = row.pct_rank # 레이블은 자료의 순서(rank)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"features\": features,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "# 순서를 가져옴\n",
        "def get_ranks(base: pd.Series, derived: List[str]) -> List[str]:\n",
        "    return [base.index(d) for d in derived]\n",
        "\n",
        "# 직렬화? 데이터 포맷을 바꿈\n",
        "def _serialize_sample(\n",
        "    input_ids: np.array,\n",
        "    attention_mask: np.array,\n",
        "    feature: np.float64,\n",
        "    label: np.float64,\n",
        ") -> bytes:\n",
        "    feature = {\n",
        "        \"input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
        "        \"attention_mask\": tf.train.Feature(\n",
        "            int64_list=tf.train.Int64List(value=attention_mask)\n",
        "        ),\n",
        "        \"feature\": tf.train.Feature(float_list=tf.train.FloatList(value=[feature])),\n",
        "        \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\n",
        "    }\n",
        "    sample = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return sample.SerializeToString()\n",
        "\n",
        "\n",
        "def serialize(\n",
        "    input_ids: np.array,\n",
        "    attention_mask: np.array,\n",
        "    features: np.array,\n",
        "    labels: np.array,\n",
        "    path: str,\n",
        ") -> None:\n",
        "    with tf.io.TFRecordWriter(path) as writer:\n",
        "        for args in zip(input_ids, attention_mask, features, labels):\n",
        "            writer.write(_serialize_sample(*args))"
      ],
      "metadata": {
        "id": "z_2EVAQtHQSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect Data"
      ],
      "metadata": {
        "id": "o67r4k2vcPED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = glob.glob(os.path.join(INPUT_PATH, \"train\", \"*.json\"))\n",
        "if LIMIT is not None:\n",
        "    paths = paths[:LIMIT]\n",
        "# 노트북 읽어옴\n",
        "df = (\n",
        "    pd.concat([read_notebook(x) for x in tqdm(paths, desc=\"Concat\")])\n",
        "    .set_index(\"id\", append=True) # id열을 index로 지정\n",
        "    .swaplevel() # Multi Index에서 두 인덱스의 순서를 변경하는 메서드\n",
        "    .sort_index(level=\"id\", sort_remaining=False) # id를 기준으로 정렬\n",
        ")\n",
        "\n",
        "# 셀 순서 읽어옴\n",
        "df_orders = pd.read_csv(\n",
        "    os.path.join(INPUT_PATH, \"train_orders.csv\"),\n",
        "    index_col=\"id\",\n",
        "    squeeze=True,\n",
        ").str.split()\n",
        "df_orders_ = df_orders.to_frame().join(\n",
        "    df.reset_index(\"cell_id\").groupby(\"id\")[\"cell_id\"].apply(list),\n",
        "    how=\"right\",\n",
        ")\n",
        "\n",
        "# 셀 순서 가져옴\n",
        "ranks = {}\n",
        "for id_, cell_order, cell_id in df_orders_.itertuples():\n",
        "    ranks[id_] = {\"cell_id\": cell_id, \"rank\": get_ranks(cell_order, cell_id)}\n",
        "df_ranks = (\n",
        "    pd.DataFrame.from_dict(ranks, orient=\"index\") # 딕셔너리를 입력받아 DataFrame을 반환, orient = index로 설정하면 딕셔너리의 키를 행의 레이블로 설정\n",
        "    .rename_axis(\"id\")\n",
        "    .apply(pd.Series.explode)\n",
        "    .set_index(\"cell_id\", append=True)\n",
        ")\n",
        "\n",
        "# 상위가 있는(fork된) 노트북 불러옴\n",
        "df_ancestors = pd.read_csv(\n",
        "    os.path.join(INPUT_PATH, \"train_ancestors.csv\"), index_col=\"id\"\n",
        ")\n",
        "df = (\n",
        "    df.reset_index()\n",
        "    .merge(df_ranks, on=[\"id\", \"cell_id\"])\n",
        "    .merge(df_ancestors, on=[\"id\"])\n",
        ") # df에 각 노트북 기준으로 상위 노트북을 merge하고, 셀 기준으로 각 셀의 순서를 merge함\n",
        "\n",
        "df[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\") # 퍼센트 랭크, 1/5(다섯 중 첫번째)와 같은 표현\n",
        "df = df.sort_values(\"pct_rank\").reset_index(drop=True) # 퍼센트 랭크를 기준으로 정렬, drop=True : 기존 인덱스가 첫번째 열로 자동 삽입되지 않도록\n",
        "\n",
        "features = get_features(df) # 특성 생성\n",
        "\n",
        "df = df[df[\"cell_type\"] == \"markdown\"] # df에는 cell type이 markdown인 자료만 있음\n",
        "df = df.drop([\"rank\", \"parent_id\", \"cell_type\"], axis=1).dropna() # rank, paren_id, cell_type 행을 drop"
      ],
      "metadata": {
        "id": "jmnBMC1YcHhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Tokens & Save"
      ],
      "metadata": {
        "id": "YOtRktTsmvpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"data.csv\") # markdown 데이터프레임 csv 파일로 저장\n",
        "with open(\"features.json\", \"w\") as file:\n",
        "    json.dump(features, file) # features.json 파일에 생성된 특성 저장(코드와 MD 모두)"
      ],
      "metadata": {
        "id": "MJBrNjHlmtzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = shuffle(df, random_state=RANDOM_STATE) # MD 셀 순서 섞음\n",
        "\n",
        "# 폴드 디렉토리 연결\n",
        "for fold, (_, split) in enumerate( # (인덱스, 원소)\n",
        "    GroupKFold(K_FOLDS).split(df, groups=df[\"ancestor_id\"]) # ancestor_id에 기반한 그룹별 교차 검증, 각 분할에서 한 그룹 전체가 훈련 세트 또는 테스트 세트에 속함\n",
        "):\n",
        "    print(\"=\" * 36, f\"Fold {fold}\", \"=\" * 36)\n",
        "    fold_dir = f\"tfrec/{fold}\"\n",
        "    if not os.path.exists(fold_dir):\n",
        "        os.mkdir(fold_dir)\n",
        "\n",
        "    data = tokenize(df.iloc[split], features)\n",
        "\n",
        "# 압축된 .npz 형식의 여러 파일을 단일 파일로 저장\n",
        "    np.savez_compressed(\n",
        "        f\"raw/{fold}.npz\",\n",
        "        input_ids=data[\"input_ids\"],\n",
        "        attention_mask=data[\"attention_mask\"],\n",
        "        features=data[\"features\"],\n",
        "        labels=data[\"labels\"],\n",
        "    )\n",
        "\n",
        "# 직렬화 후 파일 저장\n",
        "    for split, index in tqdm(\n",
        "        enumerate(np.array_split(np.arange(data[\"labels\"].shape[0]), FILES_PER_FOLD)),\n",
        "        desc=f\"Saving\",\n",
        "        total=FILES_PER_FOLD,\n",
        "    ):\n",
        "        serialize(\n",
        "            input_ids=data[\"input_ids\"][index],\n",
        "            attention_mask=data[\"attention_mask\"][index],\n",
        "            features=data[\"features\"][index],\n",
        "            labels=data[\"labels\"][index],\n",
        "            path=os.path.join(fold_dir, f\"{split:02d}-{len(index):06d}.tfrec\"),\n",
        "        )"
      ],
      "metadata": {
        "id": "rqP1n00ZoIFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}